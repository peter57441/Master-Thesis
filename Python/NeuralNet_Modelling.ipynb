{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option prices and BS-Price are divided by K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CheckAccuracy(y_true, y_pred):\n",
    "    stats = dict()\n",
    "    stats['diff'] = y_true - y_pred\n",
    "    stats['rmse'] = np.sqrt(np.mean(stats['diff']**2))\n",
    "    print(\"Root Mean Squared Error:   \" , stats['rmse'])\n",
    "    stats['mape'] = np.mean(np.abs(stats['diff'] / y_true)) \n",
    "    print(\"Mean Absolute Percentage Error:   \" , stats['mape'])\n",
    "    stats['mse'] = np.mean(stats['diff']**2)\n",
    "    print(\"Mean Squared Error:   \" , stats['mse'])\n",
    "    stats['mae'] = np.mean(np.abs(stats['diff']))\n",
    "    print(\"Mean Absolute Error:   \" , stats['mae'])\n",
    "    return stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import data\n",
    "df = pd.read_csv('C:/Users/User/Desktop/Data speciale/NeuralNet_data_v2.csv', parse_dates= True, index_col=0)\n",
    "\n",
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volatility smile\n",
    "\n",
    "import py_vollib \n",
    "from py_vollib.black_scholes.implied_volatility import implied_volatility as implied_volatility\n",
    "\n",
    "# calculate the implied volatility of the options using the Black-Scholes formula\n",
    "\n",
    "iv = []\n",
    "for i in range(len(df)):\n",
    "    intrinsic_value = max(df['Stock (S)'][i] - df['Strike (K)'][i], 0)  # For call option\n",
    "    if df['price'][i] < intrinsic_value:\n",
    "        iv.append(np.nan)  # Mark as NaN if price is below intrinsic value\n",
    "    else:\n",
    "        try:\n",
    "            iv.append(implied_volatility(\n",
    "                df['price'][i],\n",
    "                df['Stock (S)'][i],\n",
    "                df['Strike (K)'][i],\n",
    "                df['Time to maturity (T)'][i],\n",
    "                df['Risk free rate (r)'][i],\n",
    "                flag='c'\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            iv.append(np.nan)  # Handle other exceptions gracefully\n",
    "\n",
    "df['implied volatility'] = iv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select the rows where year is greater than 2022 \n",
    "#df = df[df['Year'] >= 2022]\n",
    "\n",
    "df = df.sort_index()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "unique_dates = df.index.unique()\n",
    "\n",
    "# Split into train 80% train 10% val 10% test based on index\n",
    "#train_size = int(0.80 * len(df))\n",
    "#val_size = int(0.10 * len(df))\n",
    "train_size = int(0.80 * len(unique_dates))\n",
    "val_size = int(0.10 * len(unique_dates))\n",
    "\n",
    "# Get unique dates for each split\n",
    "train_dates = unique_dates[:train_size]\n",
    "val_dates = unique_dates[train_size:train_size + val_size]\n",
    "test_dates = unique_dates[train_size:]\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset by the determined dates\n",
    "X_train = df.loc[train_dates, ['Moneyness (M)', 'Time to maturity (T)', 'Risk free rate (r)', \n",
    "                               'BS (vol10)', 'BS (vol30)', 'BS (vol60)', 'BS GARCH', 'BS VIX',\n",
    "                               'vol10', 'vol30', 'vol60', 'GARCH', 'VIX', 'Year', 'Strike (K)','tchi','shtint','gdpce']].values\n",
    "X_val = df.loc[val_dates, ['Moneyness (M)', 'Time to maturity (T)', 'Risk free rate (r)', \n",
    "                               'BS (vol10)', 'BS (vol30)', 'BS (vol60)', 'BS GARCH', 'BS VIX',\n",
    "                               'vol10', 'vol30', 'vol60', 'GARCH', 'VIX', 'Year', 'Strike (K)','tchi','shtint','gdpce']].values\n",
    "X_test = df.loc[test_dates, ['Moneyness (M)', 'Time to maturity (T)', 'Risk free rate (r)', \n",
    "                               'BS (vol10)', 'BS (vol30)', 'BS (vol60)', 'BS GARCH', 'BS VIX',\n",
    "                               'vol10', 'vol30', 'vol60', 'GARCH', 'VIX', 'Year', 'Strike (K)','tchi','shtint','gdpce']].values\n",
    "\n",
    "\n",
    "\n",
    "y_train = df.loc[train_dates, ['Option Price (C)']].values\n",
    "y_val = df.loc[val_dates, ['Option Price (C)']].values\n",
    "y_test = df.loc[test_dates, ['Option Price (C)']].values\n",
    "strike_train = X_train[:, -1]\n",
    "strike_val = X_val[:, -1]\n",
    "strike_test = X_test[:, -1]\n",
    "year_train = X_train[:, -2]\n",
    "year_val = X_val[:, -2]\n",
    "year_test = X_test[:, -2]\n",
    "BSM_train = X_train[:, 3:8]\n",
    "BSM_val = X_val[:, 3:8]\n",
    "BSM_test = X_test[:, 3:8]\n",
    "vol_train = X_train[:, 8:13]\n",
    "vol_val = X_val[:, 8:13]\n",
    "vol_test = X_test[:, 8:13]\n",
    "X_train = X_train[:, 0:3]\n",
    "X_val = X_val[:, 0:3]\n",
    "X_test = X_test[:, 0:3]\n",
    "# select the last 3 columns in x_train, x_val and x_test\n",
    "macro_train = X_train[:, -3:]\n",
    "macro_val = X_val[:, -3:]\n",
    "macro_test = X_test[:, -3:]\n",
    "\n",
    "\n",
    "\n",
    "X_train_val = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train_val = np.concatenate((y_train, y_val), axis=0)\n",
    "vol_train_val = np.concatenate((vol_train, vol_val), axis=0)\n",
    "macro_train_val = np.concatenate((macro_train, macro_val), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define the sizes for each split\n",
    "train_size = int(0.80 * len(df))\n",
    "val_size = int(0.10 * len(df))\n",
    "test_size = len(df) - train_size - val_size  # Remaining data for testing\n",
    "\n",
    "# Split the data based on the calculated sizes\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:train_size + val_size]\n",
    "test_df = df.iloc[train_size + val_size:]\n",
    "\n",
    "# Selecting features and targets for each dataset\n",
    "features = ['Moneyness (M)', 'Time to maturity (T)', 'Risk free rate (r)', \n",
    "            'BS (vol10)', 'BS (vol30)', 'BS (vol60)', 'BS GARCH', 'BS VIX',\n",
    "            'vol10', 'vol30', 'vol60', 'GARCH', 'VIX', 'Year', 'Strike (K)']\n",
    "target = ['Option Price (C)']\n",
    "\n",
    "# Separate X and y for each split\n",
    "X_train = train_df[features].values\n",
    "X_val = val_df[features].values\n",
    "X_test = test_df[features].values\n",
    "\n",
    "y_train = train_df[target].values\n",
    "y_val = val_df[target].values\n",
    "y_test = test_df[target].values\n",
    "\n",
    "# Extract specific columns for additional variables as required\n",
    "strike_train, strike_val, strike_test = X_train[:, -1], X_val[:, -1], X_test[:, -1]\n",
    "year_train, year_val, year_test = X_train[:, -2], X_val[:, -2], X_test[:, -2]\n",
    "BSM_train, BSM_val, BSM_test = X_train[:, 3:8], X_val[:, 3:8], X_test[:, 3:8]\n",
    "vol_train, vol_val, vol_test = X_train[:, 8:13], X_val[:, 8:13], X_test[:, 8:13]\n",
    "\n",
    "# Narrow down to the first three columns for X_train, X_val, and X_test as desired\n",
    "X_train = X_train[:, :3]\n",
    "X_val = X_val[:, :3]\n",
    "X_test = X_test[:, :3]\n",
    "\n",
    "# (Optional) Combine train and validation data for full training/validation set\n",
    "X_train_val = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train_val = np.concatenate((y_train, y_val), axis=0)\n",
    "vol_train_val = np.concatenate((vol_train, vol_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy of the BSM models on the test set\n",
    "\n",
    "CheckAccuracy(y_test.flatten(), BSM_test[:, 0].flatten())\n",
    "CheckAccuracy(y_test.flatten(), BSM_test[:, 1].flatten())\n",
    "CheckAccuracy(y_test.flatten(), BSM_test[:, 2].flatten())\n",
    "CheckAccuracy(y_test.flatten(), BSM_test[:, 3].flatten())\n",
    "CheckAccuracy(y_test.flatten(), BSM_test[:, 4].flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train_dates into a numpy array\n",
    "train_dates = np.array(train_dates)\n",
    "test_dates = np.array(test_dates)\n",
    "val_dates = np.array(val_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_val_scaled = scaler.transform(X_train_val)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_val_scaled = scaler_y.transform(y_val)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "y_train_val_scaled = scaler_y.transform(y_train_val)\n",
    "\n",
    "scaler_vol = MinMaxScaler()\n",
    "vol_train_scaled = scaler_vol.fit_transform(vol_train)\n",
    "vol_val_scaled = scaler_vol.transform(vol_val)\n",
    "vol_test_scaled = scaler_vol.transform(vol_test)\n",
    "vol_train_val_scaled = scaler_vol.transform(vol_train_val)\n",
    "\n",
    "scaler_macro = MinMaxScaler()\n",
    "macro_train_scaled = scaler_macro.fit_transform(macro_train)\n",
    "macro_val_scaled = scaler_macro.transform(macro_val)\n",
    "macro_test_scaled = scaler_macro.transform(macro_test)\n",
    "macro_train_val_scaled = scaler_macro.transform(macro_train_val)\n",
    "\n",
    "#X_train_sc = np.append(X_train_scaled, np.resize(vol_train_scaled[:,0], (len(vol_train_scaled),1)), axis=1)\n",
    "#X_val_sc = np.append(X_val_scaled, np.resize(vol_val_scaled[:,0], (len(vol_val_scaled),1)), axis=1)\n",
    "#X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,0], (len(vol_test_scaled),1)), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exponential decay schedule\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=4000,   # Adjust this to suit your data size\n",
    "    decay_rate=0.96,     # The rate at which the learning rate decays\n",
    "    staircase=True       # If True, the learning rate decays in discrete steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = create_model(num_layers=4, nodes=128, dropout_rate=True, learning_rate_schedule=0.001)\n",
    "\n",
    "\n",
    "X_train_val_sc = np.append(X_train_val, np.resize(vol_train_val[:,4], (len(vol_train_val),1)), axis=1)\n",
    "X_test_sc = np.append(X_test, np.resize(vol_test[:,4], (len(vol_test),1)), axis=1)\n",
    "X_train_sc = np.append(X_train, np.resize(vol_train[:,4], (len(vol_train),1)), axis=1)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_sc, y_train_scaled, epochs=2, batch_size=64, verbose=1)\n",
    "\n",
    "#y_test_pred = model.predict(X_test_sc).flatten()\n",
    "\n",
    "\n",
    "y_test_pred = scaler_y.inverse_transform(model.predict(X_test_sc)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "#model.save('C:/Users/User/Desktop/Data speciale/NeuralNetModels/NeuralNet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exponential decay schedule\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=4000,   # Adjust this to suit your data size\n",
    "    decay_rate=0.96,     # The rate at which the learning rate decays\n",
    "    staircase=True       # If True, the learning rate decays in discrete steps\n",
    ")\n",
    "\n",
    "model = create_model(num_layers=3, nodes=64, dropout_rate=True, learning_rate_schedule=lr_schedule)\n",
    "\n",
    "\n",
    "X_train_sc = np.append(X_train_scaled, np.resize(vol_train_scaled[:,4], (len(vol_train_scaled),1)), axis=1)\n",
    "X_val_sc = np.append(X_val_scaled, np.resize(vol_val_scaled[:,4], (len(vol_val_scaled),1)), axis=1)\n",
    "X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,4], (len(vol_test_scaled),1)), axis=1)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_sc, y_train_scaled, epochs=20, batch_size=1024, verbose=1)\n",
    "\n",
    "\n",
    "y_val_pred = scaler_y.inverse_transform(model.predict(X_val_sc)).flatten()\n",
    "y_test_pred = scaler_y.inverse_transform(model.predict(X_test_sc)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# calculate the accuracy\n",
    "#stats_val = CheckAccuracy(y_val.flatten(), y_val_pred.flatten())\n",
    "\n",
    "stats_test = CheckAccuracy(y_test.flatten(), y_test_pred.flatten())\n",
    "\n",
    "\n",
    "# check the accuracy of the BS vol10 model\n",
    "#stats_val_BSM = CheckAccuracy(y_val.flatten(), BSM_val[:,4].flatten())\n",
    "\n",
    "stats_test_BSM = CheckAccuracy(y_test.flatten(), BSM_test[:,4].flatten())\n",
    "\n",
    "# print the results\n",
    "#print('Val set:')\n",
    "#print('RMSE:', stats_val['rmse'])\n",
    "#print('MAPE:', stats_val['mape'])\n",
    "#print('BS VIX RMSE:', stats_val_BSM['rmse'])\n",
    "#print('BS VIX MAPE:', stats_val_BSM['mape'])\n",
    "print(' ')\n",
    "print('Test set:')\n",
    "print('RMSE:', stats_test['rmse'])\n",
    "print('MAPE:', stats_test['mape'])\n",
    "print('BS VIX RMSE:', stats_test_BSM['rmse'])\n",
    "print('BS VIX MAPE:', stats_test_BSM['mape'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build a model that has 4 hidden layers with 100 nodes each the first layer is leaky relu, then ELu, Relu, Elu and finanlly an exponential output layer\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "# First hidden layer with LeakyReLU\n",
    "model.add(Dense(100, input_dim=4))\n",
    "model.add(LeakyReLU(alpha=0.01))  # LeakyReLU with alpha=0.01 (default value)\n",
    "model.add(Dropout(0.2))\n",
    "# Second hidden layer with ELU\n",
    "model.add(Dense(100, activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "# Third hidden layer with ReLU\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# Fourth hidden layer with ELU\n",
    "model.add(Dense(100, activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "# Output layer with exponential activation\n",
    "model.add(Dense(1, activation='exponential'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "\n",
    "X_train_val = np.append(X_train_val, np.resize(vol_train_val[:,4], (len(vol_train_val),1)), axis=1)\n",
    "X_test = np.append(X_test, np.resize(vol_test[:,4], (len(vol_test),1)), axis=1)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_val, y_train_val, epochs=30, batch_size=4096, verbose=1)\n",
    "\n",
    "#y_test_pred = model.predict(X_test_sc).flatten()\n",
    "\n",
    "\n",
    "y_test_pred = model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'n_hidden': [1, 2, 3],\n",
    "    'n_neurons': [32, 64, 128],\n",
    "    'use_dropout': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(num_layers=2, nodes=64, dropout_rate=True, learning_rate_schedule=None):\n",
    "    model = Sequential()\n",
    "\n",
    "      \n",
    "    # Input Layer\n",
    "    model.add(Dense(nodes, input_dim=5, activation='relu', kernel_regularizer=l2(0.0001)))\n",
    "\n",
    "    if dropout_rate == True:\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    # Hidden Layers\n",
    "    for _ in range(num_layers-1):\n",
    "        model.add(Dense(nodes, activation='relu', kernel_regularizer=l2(0.0001)))\n",
    "        if dropout_rate == True:\n",
    "            model.add(Dropout(0.2))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(1, activation='linear')) # Output layer is the price of the option. This is non negative as the price of an option cannot be negative\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate_schedule)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "volatility_model_list = ['vol10', 'vol30', 'vol60', 'GARCH', 'VIX']\n",
    "errorList_csv = ['diff_train.csv', 'diff_val.csv']\n",
    "\n",
    "# build the model for the hyperparameter grid and a specific volatility model\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "y_val = y_val.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "for volatility in range(len(volatility_model_list)):\n",
    "    for layers in param_grid['n_hidden']:\n",
    "        for nodes in param_grid['n_neurons']:\n",
    "            for dropout in param_grid['use_dropout']:\n",
    "                start_time = time.time()\n",
    "\n",
    "                initial_learning_rate = 0.001\n",
    "                lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=initial_learning_rate,\n",
    "                    decay_steps=4000,   # Adjust this to suit your data size\n",
    "                    decay_rate=0.96,     # The rate at which the learning rate decays\n",
    "                    staircase=True       # If True, the learning rate decays in discrete steps\n",
    "                )\n",
    "\n",
    "                # Build model (assuming `create_model` is your model creation function)\n",
    "                model = create_model(num_layers=layers, nodes=nodes, dropout_rate=dropout, learning_rate_schedule=lr_schedule)\n",
    "\n",
    "                # Prepare training and validation data without using `tf.data`\n",
    "                X_train_sc = np.append(X_train_scaled, np.resize(vol_train_scaled[:, volatility], (len(vol_train_scaled), 1)), axis=1)\n",
    "                X_val_sc = np.append(X_val_scaled, np.resize(vol_val_scaled[:, volatility], (len(vol_val_scaled), 1)), axis=1)\n",
    "                X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:, volatility], (len(vol_test_scaled), 1)), axis=1)\n",
    "                #X_train_val_sc = np.append(X_train_val_scaled, np.resize(vol_train_val_scaled[:, volatility], (len(vol_train_val_scaled), 1)), axis=1)\n",
    "                \n",
    "                \n",
    "                model.fit(X_train_sc, y_train_scaled, epochs=20, batch_size=1024, verbose=0)\n",
    "\n",
    "                # After training, validate by predicting on the validation set\n",
    "                y_train_pred = scaler_y.inverse_transform(np.float64(model.predict(X_train_sc))).flatten()\n",
    "                y_val_pred = scaler_y.inverse_transform(np.float64(model.predict(X_val_sc))).flatten()\n",
    "\n",
    "\n",
    "                # ANN Evaluation Metrics\n",
    "                diff_train = y_train - y_train_pred\n",
    "                mse_train = np.mean(diff_train**2)\n",
    "                mape_train = np.mean(np.abs(diff_train / y_train))\n",
    "                diff_val = y_val - y_val_pred\n",
    "                mse_val = np.mean(diff_val**2)\n",
    "                mape_val = np.mean(np.abs(diff_val / y_val))\n",
    "          \n",
    "\n",
    "                # BSM Evaluation Metrics\n",
    "                #diff_BSM_train = y_train - BSM_train[:, volatility]\n",
    "                #mse_BSM_train = np.mean(diff_BSM_train**2)\n",
    "                #mape_BSM_train = np.mean(np.abs(diff_BSM_train / y_train))\n",
    "                #diff_BSM_val = y_val - BSM_val[:, volatility]\n",
    "                #mse_BSM_val = np.mean(diff_BSM_val**2)\n",
    "                #mape_BSM_val = np.mean(np.abs(diff_BSM_val / y_val))\n",
    "\n",
    "                # Print the results\n",
    "                print('Model completed:', volatility_model_list[volatility])\n",
    "                print('Layers:', layers, 'Nodes:', nodes, 'Dropout:', dropout)\n",
    "                print('ANN Train MSE:', mse_train, 'MAPE:', mape_train)\n",
    "                print('ANN Val MSE:', mse_val, 'MAPE:', mape_val)\n",
    "                # print('BSM Train MSE:', mse_BSM_train, 'MAPE:', mape_BSM_train)\n",
    "                # print('BSM Val MSE:', mse_BSM_val, 'MAPE:', mape_BSM_val)\n",
    "\n",
    "                # Save the model\n",
    "                print('Saving Model')\n",
    "                model.save('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Saved models/V2/' +\n",
    "                           f'model_{volatility_model_list[volatility]}_layers{layers}_nodes{nodes}_dropout{dropout}.h5')\n",
    "\n",
    "                # Save ANN Evaluation Metrics to CSV\n",
    "                df = pd.DataFrame({\n",
    "                    'Model': ['ANN']*2,\n",
    "                    'Volatility Model': [volatility_model_list[volatility]]*2,\n",
    "                    'Layers': [layers]*2,\n",
    "                    'Nodes': [nodes]*2,\n",
    "                    'Dropout': [dropout]*2,\n",
    "                    'MSE': [mse_train, mse_val],\n",
    "                    'MAPE': [mape_train, mape_val],\n",
    "                    'Val/Test': ['Train', 'Val']\n",
    "                })\n",
    "                df.to_csv('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Results/ANN_results_newMethod_v3.csv',\n",
    "                          mode='a', header=False, index=False)\n",
    "\n",
    "                # Save residuals\n",
    "                #print('Saving Residuals')\n",
    "                #errorList = [diff_train, diff_val]\n",
    "                #for j in range(len(errorList)):\n",
    "                #    df = pd.DataFrame(errorList[j])\n",
    "                #    df.to_csv(f'C:/Users/User/Desktop/Data speciale/NeuralNetModels/Residuals/'\n",
    "                #              f'model_{volatility_model_list[volatility]}_layers{layers}_nodes{nodes}_dropout{dropout}_{errorList_csv[j]}.csv',\n",
    "                #              index=False)\n",
    "\n",
    "                print('Model saved')\n",
    "                end_time = time.time()\n",
    "                execution_time = end_time - start_time\n",
    "                print('Execution time:', execution_time)\n",
    "                    \n",
    "                \n",
    "                \n",
    "                        \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for the macroeconomic variables\n",
    "\n",
    "# Define the exponential decay schedule\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=4000,   # Adjust this to suit your data size\n",
    "    decay_rate=0.96,     # The rate at which the learning rate decays\n",
    "    staircase=True       # If True, the learning rate decays in discrete steps\n",
    ")\n",
    "\n",
    "model = create_model(num_layers=3, nodes=64, dropout_rate=False, learning_rate_schedule=lr_schedule)\n",
    "\n",
    "X_train_val_sc = np.append(X_train_val_scaled, np.resize(vol_train_val_scaled[:, 4], (len(vol_train_val_scaled), 1)), axis=1)\n",
    "# add the macroeconomic variables to the X_train_val_sc\n",
    "# 0 = tchi, 1 = shtint, 2 = gdpce\n",
    "X_train_val_sc = np.append(X_train_val_sc, np.resize(macro_train_val_scaled[:, 2], (len(macro_train_val_scaled), 1)), axis=1)\n",
    "\n",
    "X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:, 4], (len(vol_test_scaled), 1)), axis=1)\n",
    "\n",
    "X_test_sc = np.append(X_test_sc, np.resize(macro_test_scaled[:, 2], (len(macro_test_scaled), 1)), axis=1)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_val_sc, y_train_val_scaled, epochs=30, batch_size=1024, verbose=1)\n",
    "\n",
    "y_test_pred = scaler_y.inverse_transform(model.predict(X_test_sc)).flatten()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ANN_results = pd.read_csv('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Results/ANN_Results.csv')\n",
    "\n",
    "# load the best model\n",
    "model_VIX = tf.keras.models.load_model('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Saved models/V2/model_VIX_layers3_nodes64_dropoutFalse.h5')\n",
    "\n",
    "model_GARCH = tf.keras.models.load_model('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Saved models/V2/model_GARCH_layers1_nodes128_dropoutTrue.h5')\n",
    "\n",
    "model_vol60 = tf.keras.models.load_model('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Saved models/V2/model_vol60_layers1_nodes128_dropoutFalse.h5')\n",
    "\n",
    "model_vol30 = tf.keras.models.load_model('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Saved models/V2/model_vol30_layers2_nodes32_dropoutFalse.h5')\n",
    "\n",
    "model_vol10 = tf.keras.models.load_model('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Saved models/V2/model_vol10_layers1_nodes32_dropoutFalse.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define the sizes for each split\n",
    "train_size = int(0.80 * len(df))\n",
    "val_size = int(0.10 * len(df))\n",
    "test_size = len(df) - train_size - val_size  # Remaining data for testing\n",
    "\n",
    "# Split the data based on the calculated sizes\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:train_size + val_size]\n",
    "test_df = df.iloc[train_size + val_size:]\n",
    "\n",
    "# Selecting features and targets for each dataset\n",
    "features = ['Moneyness (M)', 'Time to maturity (T)', 'Risk free rate (r)', \n",
    "            'BS (vol10)', 'BS (vol30)', 'BS (vol60)', 'BS GARCH', 'BS VIX',\n",
    "            'vol10', 'vol30', 'vol60', 'GARCH', 'VIX', 'Year', 'Strike (K)']\n",
    "target = ['Option Price (C)']\n",
    "\n",
    "# Separate X and y for each split\n",
    "X_train = train_df[features].values\n",
    "X_val = val_df[features].values\n",
    "X_test = test_df[features].values\n",
    "\n",
    "y_train = train_df[target].values\n",
    "y_val = val_df[target].values\n",
    "y_test = test_df[target].values\n",
    "\n",
    "# Extract specific columns for additional variables as required\n",
    "strike_train, strike_val, strike_test = X_train[:, -1], X_val[:, -1], X_test[:, -1]\n",
    "year_train, year_val, year_test = X_train[:, -2], X_val[:, -2], X_test[:, -2]\n",
    "BSM_train, BSM_val, BSM_test = X_train[:, 3:8], X_val[:, 3:8], X_test[:, 3:8]\n",
    "vol_train, vol_val, vol_test = X_train[:, 8:13], X_val[:, 8:13], X_test[:, 8:13]\n",
    "\n",
    "# Narrow down to the first three columns for X_train, X_val, and X_test as desired\n",
    "X_train = X_train[:, :3]\n",
    "X_val = X_val[:, :3]\n",
    "X_test = X_test[:, :3]\n",
    "\n",
    "# (Optional) Combine train and validation data for full training/validation set\n",
    "X_train_val = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train_val = np.concatenate((y_train, y_val), axis=0)\n",
    "vol_train_val = np.concatenate((vol_train, vol_val), axis=0)\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_val_scaled = scaler.transform(X_train_val)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_val_scaled = scaler_y.transform(y_val)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "y_train_val_scaled = scaler_y.transform(y_train_val)\n",
    "\n",
    "scaler_vol = MinMaxScaler()\n",
    "vol_train_scaled = scaler_vol.fit_transform(vol_train)\n",
    "vol_val_scaled = scaler_vol.transform(vol_val)\n",
    "vol_test_scaled = scaler_vol.transform(vol_test)\n",
    "vol_train_val_scaled = scaler_vol.transform(vol_train_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the option prices for the test set for each model\n",
    "\n",
    "X_train_sc = np.append(X_train_scaled, np.resize(vol_train_scaled[:,4], (len(vol_train_scaled),1)), axis=1)\n",
    "X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,4], (len(vol_test_scaled),1)), axis=1)\n",
    "#model_VIX = create_model(num_layers=2, nodes=128, dropout_rate=True, learning_rate=0.001)\n",
    "#model_VIX.fit(X_train_sc, y_train_scaled, epochs=30, batch_size=4096, verbose=0)\n",
    "y_pred_VIX = scaler_y.inverse_transform(np.float64(model_VIX.predict(X_test_sc))).flatten()\n",
    "\n",
    "X_train_sc = np.append(X_train_scaled, np.resize(vol_train_scaled[:,3], (len(vol_train_scaled),1)), axis=1)\n",
    "X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,3], (len(vol_test_scaled),1)), axis=1)\n",
    "#model_GARCH = create_model(num_layers=2, nodes=64, dropout_rate=True, learning_rate=0.001)\n",
    "#model_GARCH.fit(X_train_sc, y_train_scaled, epochs=30, batch_size=4096, verbose=0)\n",
    "y_pred_GARCH = scaler_y.inverse_transform(np.float64(model_GARCH.predict(X_test_sc))).flatten()\n",
    "\n",
    "X_train_sc = np.append(X_train_scaled, np.resize(vol_train_scaled[:,2], (len(vol_train_scaled),1)), axis=1)\n",
    "X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,2], (len(vol_test_scaled),1)), axis=1)\n",
    "#model_vol60 = create_model(num_layers=2, nodes=128, dropout_rate=True, learning_rate=0.001)\n",
    "#model_vol60.fit(X_train_sc, y_train_scaled, epochs=30, batch_size=4096, verbose=0)\n",
    "y_pred_vol60 = scaler_y.inverse_transform(np.float64(model_vol60.predict(X_test_sc))).flatten()\n",
    "\n",
    "X_train_sc = np.append(X_train_scaled, np.resize(vol_train_scaled[:,1], (len(vol_train_scaled),1)), axis=1)\n",
    "X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,1], (len(vol_test_scaled),1)), axis=1)\n",
    "#model_vol30 = create_model(num_layers=2, nodes=32, dropout_rate=True, learning_rate=0.001)\n",
    "#model_vol30.fit(X_train_sc, y_train_scaled, epochs=30, batch_size=4096, verbose=0)\n",
    "y_pred_vol30 = scaler_y.inverse_transform(np.float64(model_vol30.predict(X_test_sc))).flatten()\n",
    "\n",
    "X_train_sc = np.append(X_train_scaled, np.resize(vol_train_scaled[:,0], (len(vol_train_scaled),1)), axis=1)\n",
    "X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,0], (len(vol_test_scaled),1)), axis=1)\n",
    "#model_vol10 = create_model(num_layers=2, nodes=32, dropout_rate=False, learning_rate=0.001)\n",
    "#model_vol10.fit(X_train_sc, y_train_scaled, epochs=30, batch_size=4096, verbose=0)\n",
    "y_pred_vol10 = scaler_y.inverse_transform(np.float64(model_vol10.predict(X_test_sc))).flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate the accuracy for each model\n",
    "print('ANN')\n",
    "print('VIX')\n",
    "stats_VIX = CheckAccuracy(y_test.flatten(), y_pred_VIX)\n",
    "print('GARCH')\n",
    "stats_GARCH = CheckAccuracy(y_test.flatten(), y_pred_GARCH)\n",
    "print('vol60')\n",
    "stats_vol60 = CheckAccuracy(y_test.flatten(), y_pred_vol60)\n",
    "print('vol30')\n",
    "stats_vol30 = CheckAccuracy(y_test.flatten(), y_pred_vol30)\n",
    "print('vol10')\n",
    "stats_vol10 = CheckAccuracy(y_test.flatten(), y_pred_vol10)\n",
    "\n",
    "# calculate the accuracy for the BS model \n",
    "print('BSM')\n",
    "print('VIX')\n",
    "stats_BSM_VIX = CheckAccuracy(y_test.flatten(), BSM_test[:,4].flatten())\n",
    "print('GARCH')\n",
    "stats_BSM_GARCH = CheckAccuracy(y_test.flatten(), BSM_test[:,3].flatten())\n",
    "print('vol60')\n",
    "stats_BSM_vol60 = CheckAccuracy(y_test.flatten(), BSM_test[:,2].flatten())\n",
    "print('vol30')\n",
    "stats_BSM_vol30 = CheckAccuracy(y_test.flatten(), BSM_test[:,1].flatten())\n",
    "print('vol10')\n",
    "stats_BSM_vol10 = CheckAccuracy(y_test.flatten(), BSM_test[:,0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the results in on dataframe with X_test and y_test and the predictions and attach it to the original dataframe\n",
    "\n",
    "#df_val = df.loc[val_dates, ['Moneyness (M)', 'Time to maturity (T)', 'Risk free rate (r)',\n",
    "#                            'BS (vol10)', 'BS (vol30)', 'BS (vol60)', 'BS GARCH', 'BS VIX',\n",
    "#                            'vol10', 'vol30', 'vol60', 'GARCH', 'VIX', 'Year', 'Strike (K)', 'Option Price (C)', 'Stock (S)']]\n",
    "#df_val['ANN VIX'] = y_pred_VIX\n",
    "#df_val['ANN GARCH'] = y_pred_GARCH\n",
    "#df_val['ANN vol60'] = y_pred_vol60\n",
    "#df_val['ANN vol30'] = y_pred_vol30\n",
    "#df_val['ANN vol10'] = y_pred_vol10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# do the same for train set \n",
    "\n",
    "#df_train = df.loc[train_dates, ['Moneyness (M)', 'Time to maturity (T)', 'Risk free rate (r)',\n",
    "#                            'BS (vol10)', 'BS (vol30)', 'BS (vol60)', 'BS GARCH', 'BS VIX',\n",
    "#                            'vol10', 'vol30', 'vol60', 'GARCH', 'VIX', 'Year', 'Strike (K)', 'Option Price (C)', 'Stock (S)']]\n",
    "#df_train['ANN VIX'] = y_pred_VIX_train\n",
    "#df_train['ANN GARCH'] = y_pred_GARCH_train\n",
    "#df_train['ANN vol60'] = y_pred_vol60_train\n",
    "#df_train['ANN vol30'] = y_pred_vol30_train\n",
    "#df_train['ANN vol10'] = y_pred_vol10_train\n",
    "\n",
    "\n",
    "\n",
    "# save the results to a csv file\n",
    "#df_val.to_csv('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Results/NN_results_test.csv')\n",
    "#df_train.to_csv('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Results/NN_results_train.csv')\n",
    "\n",
    "test_df['ANN VIX'] = y_pred_VIX\n",
    "test_df['ANN GARCH'] = y_pred_GARCH\n",
    "test_df['ANN vol60'] = y_pred_vol60\n",
    "test_df['ANN vol30'] = y_pred_vol30\n",
    "test_df['ANN vol10'] = y_pred_vol10\n",
    "\n",
    "\n",
    "test_df.to_csv('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Results/NN_results_test_v2.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import every model in directory on the computer and predict the option prices for the test set then store the results in a csv file\n",
    "import os\n",
    "\n",
    "model_dir = 'C:/Users/User/Desktop/Data speciale/NeuralNetModels'\n",
    "model_list = os.listdir(model_dir)\n",
    "\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "for model_name in model_list:\n",
    "    model = tf.keras.models.load_model(model_dir + '/' + model_name)\n",
    "    \n",
    "    # if model name contains 'vol10' then use the vol10 volatility model\n",
    "    if 'vol10' in model_name:\n",
    "        X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,0], (len(vol_test_scaled),1)), axis=1)\n",
    "    elif 'vol30' in model_name:\n",
    "        X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,1], (len(vol_test_scaled),1)), axis=1)\n",
    "    elif 'vol60' in model_name:\n",
    "        X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,2], (len(vol_test_scaled),1)), axis=1)\n",
    "    elif 'GARCH' in model_name:\n",
    "        X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,3], (len(vol_test_scaled),1)), axis=1)\n",
    "    elif 'VIX' in model_name:\n",
    "        X_test_sc = np.append(X_test_scaled, np.resize(vol_test_scaled[:,4], (len(vol_test_scaled),1)), axis=1)\n",
    "\n",
    "    y_pred = scaler_y.inverse_transform(np.float64(model.predict(X_test_sc))).flatten()\n",
    "    stats = CheckAccuracy(y_test, y_pred)\n",
    "    df = pd.DataFrame({\n",
    "        'Model': [model_name]*2,\n",
    "        'MSE': [stats['mse']]*2,\n",
    "        'MAPE': [stats['mape']]*2,\n",
    "        'Train/Test': ['Train', 'Test']\n",
    "    })\n",
    "    df.to_csv('C:/Users/User/Desktop/Data speciale/NeuralNetModels/Results/ANN_results_v3_test.csv',\n",
    "              mode='a', header=False, index=False)\n",
    "    print(model_name, 'completed')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
